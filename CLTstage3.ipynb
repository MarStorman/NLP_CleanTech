{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c81ef-1d8f-4608-b7af-d1f2c44de6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages.\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5259fc0-0adf-4c35-a82a-8e241891f887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d331cdb-8798-43be-9867-43176d6db060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bfbbcf-2433-453a-95d0-7cc89ffbd360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8babf5-f8ae-4b93-9260-4c32e5a67a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_folder = Path(\"/content/drive/MyDrive/HSLU/CTA/\")\n",
    "cleantech = data_folder / \"cleantech_media_dataset_v1_20231109.csv\"\n",
    "# Inspect the data frame\n",
    "df3 = pd.read_csv(cleantech)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364e9b5-e278-4218-8e4a-7d47966fb3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing html tags\n",
    "df3['content'] = df3['content'].apply(lambda x: re.sub('<[^<]+?>', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc006c-77f5-4bb6-961b-b06c0db659ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df3.drop_duplicates(subset='content', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec54523-cb50-4b78-946c-f69dd3eddb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Newlines, spaces and tabs removal\n",
    "df3['content'] = df3['content'].apply(lambda x: re.sub('\\s+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2abe7dd-6938-4a13-a835-b9a74585883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unused coloumns\n",
    "df3.drop(['Unnamed: 0','author','domain','url'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098558bc-a807-46c5-aa33-bfa725347415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d6205-adad-4c82-8ee9-d2974b712cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab51d54-c94e-4b4b-a3c8-a027d3ee3938",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3f8e4e-3454-4f03-bb4a-6b095731e1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85aff85b-896f-47d0-8fdd-f4c66e0fe2b4",
   "metadata": {},
   "source": [
    "#### Use BERT Extractive Summarizer to extract key sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf92f5f6-770c-47c6-85d3-6949183bc0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc102c8-e427-4d83-ab96-601233b068a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e3d26-eeaa-49da-8fd5-92548c4f9da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc22429-26e9-48db-9392-0e0a26170bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract Keywords\n",
    "from summarizer import Summarizer\n",
    "model = Summarizer()\n",
    "result = model(df3['content'][1], min_length=60)\n",
    "result\n",
    "# df3['key_sentences'] = df3['content'].apply(lambda x: summarizer.summarize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc94cde-6ba3-406b-a321-7b6641d9c30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d4c56-fa2c-475c-abaa-dc1fb1a53e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['content'][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fec48b-17f1-485a-9b04-8a51daa1f35f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704f8f5-6c47-43a1-b24d-8fa547171f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the last 100 rows\n",
    "n=100\n",
    "\n",
    "df_last100 = df3[-n:]\n",
    "\n",
    "# extract the ke sentences\n",
    "df_last100['key_sentences'] = df_last100['content'].apply(lambda x: model(x,min_length=30))\n",
    "\n",
    "print(f'the last {n} key sentense finished!')\n",
    "\n",
    "# save the output to a csv\n",
    "df_last100.to_csv(data_folder / \"last100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb8dd5-32d4-4144-aedf-febd1beba525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf15ed2c-8100-4b16-bc3b-1832e0849d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last100['content'][9602]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec150eb8-3c51-400e-ac36-dddc0b6c3b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last100['key_sentences'][9602]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0baa5b3-c33d-482b-98d9-31cc11ec0b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0c1f65c-cc2e-462f-be94-ee30925bef69",
   "metadata": {},
   "source": [
    "## Generate Questions and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8120ee35-df6e-4c0a-b93f-4b845a133763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fddeb55-2f8d-4b57-97b3-56512d62a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Inspect the data frame\n",
    "df_last100 = pd.read_csv('/content/drive/MyDrive/HSLU/CTA/last100.csv')\n",
    "\n",
    "# Remove unused coloumns\n",
    "df_last100.drop(['Unnamed: 0','title','date'], axis=1, inplace=True)\n",
    "df_last100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd82ab79-7dfb-4d7a-ad98-3331be797ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "933de270-ab43-452f-b918-d64061473af7",
   "metadata": {},
   "source": [
    "### USE Transformer model to generate question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59bd57-9261-4644-a66e-35766fc7129d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae01b2-66c2-46c3-bcc1-9c469fdf43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89446db8-327a-4a83-95a9-000be7d46721",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e285139-b988-4c77-8a07-012cb8f320fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/xuexi21/CTA/blob/main/qa_generator.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1800e7db-3fe7-433e-b2e9-143f5e520342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661846a-781d-4811-a54b-a922f15723a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/HSLU/CTA/qa_generator.zip -d /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99265a-9db7-4c9a-af31-c3041605c5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da95b0e-2e21-4723-af40-c56c3c653d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/question_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d1cc73-390c-4682-a049-6635c17f2fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f7f81-2fbc-48b3-bbc5-24a2ec236188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelines import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f23828-ff87-4202-8cdc-6e35dc090951",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Ensure that the GPU is available\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ff6f4-c1fd-4409-8771-0cb0dec124ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10f7f4-1bec-49ae-8fff-74b257d60c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/valhalla/t5-base-qg-hl\n",
    "qa_generator = pipeline(\"question-generation\", model=\"valhalla/t5-base-qg-hl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c6a6b-1728-4f81-9365-851dcec7da22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1911a-e76d-4237-9ae3-83d770ed0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a empty list to store the Q&A!\n",
    "gen_text_list = []\n",
    "\n",
    "# use for loop to generate Q&A\n",
    "for i, text in enumerate(df_last100['key_sentences']):\n",
    "  print(i)\n",
    "  gen_text_list.append(qa_generator(text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751563bb-cf21-46c4-8fc9-af227bf6af0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e11cbb-39a6-4de3-acce-59bd5004b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SAVE the result to df\n",
    "# gen_text_list\n",
    "df_last100['QA'] = gen_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d973a9b-c079-4875-9ffd-298816c38869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f1bbd4-d96f-4d84-8c29-0d1defe8696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the content index to check\n",
    "n = 16\n",
    "\n",
    "#PRINT THE KEY SENTENCE\n",
    "print(df_last100['key_sentences'][n])\n",
    "\n",
    "#PRINT THE QA\n",
    "print(df_last100['QA'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17a96c-dfff-41ea-9cf8-ac78e758a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the stage to csv\n",
    "data_folder = Path(\"/content/drive/MyDrive/HSLU/CTA/\")\n",
    "df_last100.to_csv(data_folder / \"last100_QA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08138175-b847-4359-9166-9b6b867f629c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0daa9bf6-c774-4aa7-beba-45768e350061",
   "metadata": {},
   "source": [
    "## Manually clean up the generated question-answer pairs to create a high-quality QA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d97e7-fa75-453f-94d7-2cd1c62fa5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b13d74-284f-42d2-86b9-edbd6ce3f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the packages\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import ast # convert the string to a list using the ast module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6790148-937c-40bc-a6d1-9ca8127d5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount the drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e28ef0-2ffd-49fe-b360-9ac8651875d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the stage data\n",
    "data_folder = Path(\"/content/drive/MyDrive/HSLU/CTA/\")\n",
    "df_qa = pd.read_csv(data_folder / \"last100_QA.csv\")\n",
    "# rename the column\n",
    "df_qa = df_qa.rename(columns={\"Unnamed: 0\": \"idx\", \"key_sentences\": \"summary\",})\n",
    "# select the usful column\n",
    "df_qa = df_qa.iloc[:, [0,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920e3c0-e055-4c84-b69c-a3d076b13325",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdcc2fc-2175-46df-bb98-8efc95c7e4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec78d8-27ab-427e-9aea-064a7afcb5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN the QA Content\n",
    "df_QA_clean = pd.DataFrame({'summary':[],'Q':[],'A':[]})\n",
    "# new_ind = 0\n",
    "for i in df_qa.index:\n",
    "    # if i == 5:\n",
    "    #   break\n",
    "    # print(f\"\\n\\n index: {i} \\n\\n\")\n",
    "\n",
    "\n",
    "    # Convert the string representation of a list to a list of dictionaries\n",
    "    qa_list = ast.literal_eval(df_qa['QA'].iloc[i])\n",
    "    # print(f\"then lenth of the QA IS {len(qa_list)}\")\n",
    "\n",
    "    for j, key_s in enumerate(qa_list):\n",
    "        if \"'\" in key_s['answer']:\n",
    "          continue\n",
    "        else:\n",
    "          # Append Dict as row to DataFrame\n",
    "          new_row = {\"summary\": df_qa[\"summary\"].iloc[i], \"Q\": key_s['question'],\"A\":key_s['answer']}\n",
    "          df_QA_clean = df_QA_clean.append(new_row, ignore_index=True)\n",
    "\n",
    "            # print(f\"index:{i}, the {j} sentence contains a single quote in the 'answer' key.\")\n",
    "            # print(f\"index:{i}, the {j} sentence is {key_s}\")\n",
    "            # print(f\"ANSWER: {key_s['answer']}\")\n",
    "            # print(f\"QUESTION: {key_s['question']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808d4cbf-0419-4489-bce3-cac2034a92c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db4ac8-d41e-4ca5-bec5-c621d1edd1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_QA_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9efdf0-fdd1-4dca-81cf-c9d1299d506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the stage to csv\n",
    "data_folder = Path(\"/content/drive/MyDrive/HSLU/CTA/\")\n",
    "df_QA_clean.to_csv(data_folder / \"last100_QA_clean.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ded1e9-bf61-4f73-8185-aaca48056e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b554586-c99a-401f-82b5-4b12ed3c3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fine-tune GPT-2 or T5 and evaluate model performance on new input data in the cleantech field.\n",
    "\n",
    "\n",
    "We used GPT 2 model in following study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23360f7d-283b-4b32-bd4f-ac8380894552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a46f5-b3ab-4e10-8608-97e425cb7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f69c3-0c36-476b-98d4-29ca84f04151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab392c25-8948-4fe0-a9cf-12b6ff063a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39479e37-9813-4a8f-a134-8d8fa646abfd",
   "metadata": {},
   "source": [
    "####  1 Load the dataset into a data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139451b7-ee0a-4d77-a8c6-6f5cbec7128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# mount the drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2899211-f6a1-4312-b29f-adae38b15fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e988d-d319-4561-bbf2-3acfc3483605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into a data frame\n",
    "data_folder = Path(\"/content/drive/MyDrive/HSLU/CTA/\")\n",
    "filename = \"last100_QA_clean.csv\"\n",
    "df = pd.read_csv(data_folder /filename)\n",
    "\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e71123-6896-4faa-b074-75e6d5db254d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64444201-83f9-4a02-afdc-9e86edd1242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR CALCULATE THE MAX TOKEN LENTH,\n",
    "\n",
    "\n",
    "test_Q = df.Q.copy()\n",
    "test_A = df.A.copy()\n",
    "print(test_Q)\n",
    "print(\"\\n\\n\")\n",
    "print(test_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40292df-48a4-4737-93f0-ce98feaba322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163ac2e-a79d-43ec-82a4-f012feadcc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the max length of the tokens in 'answer'.\n",
    "doc_lengths_A = []\n",
    "\n",
    "for text in test_A:\n",
    "\n",
    "    # get rough token count distribution\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    doc_lengths_A.append(len(tokens))\n",
    "\n",
    "doc_lengths_A = np.array(doc_lengths_A)\n",
    "# PLOT THE DENSITY\n",
    "sns.distplot(doc_lengths_A)\n",
    "\n",
    "# calculate the max length of the tokens in 'question'.\n",
    "doc_lengths_Q = []\n",
    "\n",
    "for text in test_Q:\n",
    "\n",
    "    # get rough token count distribution\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    doc_lengths_Q.append(len(tokens))\n",
    "\n",
    "doc_lengths_Q = np.array(doc_lengths_Q)\n",
    "# PLOT THE DENSITY\n",
    "sns.distplot(doc_lengths_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d85d4b-b213-4690-82db-73ac991d2ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96960d1-fc7d-42f2-869f-bea401d20400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the max token length\n",
    "doc_lengths_A.max() + doc_lengths_Q.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fee7e9-8f33-446e-bc6f-c91dcf0d2d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the avg token length\n",
    "np.average(doc_lengths_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd2ea4d-d2d9-4670-8452-3ca1faacd68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "245d917c-2167-49c7-afb4-90a867f2b98a",
   "metadata": {},
   "source": [
    "Even though these token counts won't match up to the BPE tokenizer's, I'm confident that most bios will be fit under the 60 embedding size limit for the small GPT2 model.\n",
    "\n",
    "\n",
    "#### 2 GPT2 Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f103b89c-7161-436d-b6dc-d27fa396f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT tokenizer.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40addf4-1a4e-4a35-b498-26189ed07fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108c9936-dfd5-49cf-9baf-6f74c9ce2da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The max model length is {} for this model, although the actual embedding size for GPT small is 60\".format(tokenizer.model_max_length))\n",
    "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
    "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
    "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc3579-4008-423a-bd9f-9acc9e6ce9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d7ba30d-464e-40b4-aa5f-29a3f93bd348",
   "metadata": {},
   "source": [
    "#### 3 PyTorch Datasets & Dataloaders\n",
    "\n",
    "\n",
    "I'm using the standard PyTorch approach of loading data in using a dataset class.\n",
    "\n",
    "I'm passing in the tokenizer as an argument but normally I would instantiate it within the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c3de6-54a0-44fd-9ddd-cf48fec3b777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67705f-86a5-47c1-94a6-c08d00f431d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "\n",
    "  def __init__(self, df_qa, tokenizer, gpt2_type=\"gpt2\", max_length=60):\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "    self.question_tokens = []\n",
    "    self.question_masks = []\n",
    "    self.answer_tokens = []\n",
    "\n",
    "    for index, row in df_qa.iterrows():\n",
    "\n",
    "      q_dict = tokenizer('<|startoftext|>'+ '{Question:} ' + row['Q'] + ' {Answer:} ' + row['A']+ '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "      self.question_tokens.append(torch.tensor(q_dict['input_ids']))\n",
    "      self.question_masks.append(torch.tensor(q_dict['attention_mask']))\n",
    "      # if index == 10:\n",
    "      #   break\n",
    "  def __len__(self):\n",
    "    return len(self.question_tokens)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.question_tokens[idx], self.question_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4872c9-159a-4455-ac17-cb55a9704a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0d98cb3-d2ea-4213-af3c-77af89ffd0c0",
   "metadata": {},
   "source": [
    "##### SPLIT THE DATASET for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebee50-598b-47b7-97fb-c76c93af3e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdde762-3862-4601-9d20-0ce0f9797357",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GPT2Dataset(df, tokenizer, max_length=60)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.999 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c017b82f-26cc-4509-9aee-5d2f9a91a638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f28c0b9-2281-4323-adb1-6b8f910be685",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a975d4a-e80b-46b9-8a51-4e89372f2fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99e5b7dd-eae2-4151-a14a-4a5a6ab3adc8",
   "metadata": {},
   "source": [
    "##### CEATE the dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848891f9-2b95-4253-9d92-88117a4935cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934bd664-df56-4535-8e88-77dafdc817e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f1cd87d-7479-4ed6-97ce-01ceaa2e1ff7",
   "metadata": {},
   "source": [
    "#### 4 Finetune GPT2 Language Model\n",
    "\n",
    "\n",
    "##### SET THE CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a91bee4-81e7-4899-8951-85714b7f8ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the configuration\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "# instantiate the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device = torch.device(\"cuda\")\n",
    "model.cuda()\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3d777-1f3c-4364-a7aa-4492d63e1361",
   "metadata": {},
   "source": [
    "##### set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad931499-73f1-4de4-9798-6eb7bf0db25b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff06b86-9d3d-4f1e-b324-776007e570f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some parameters I cooked up that work reasonably well\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 1e-3 # 5e-4\n",
    "warmup_steps = 0 # 1e2\n",
    "epsilon = 1e-8\n",
    "\n",
    "# this produces sample output every 100 steps\n",
    "sample_every = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8128025-1201-48be-b592-b328aedfe863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6675d175-e74f-4070-bd16-6b17ca3c4e43",
   "metadata": {},
   "source": [
    "##### set the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaac886-1940-40a9-ba27-ba588172309e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782d4b8-2c3b-48ff-acd2-ec24d500df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate,\n",
    "                  eps = epsilon\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34814fc-a506-41c9-9489-3eac387ce928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26161252-c8cb-410b-9204-8134eb2abe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SET THE TRAINING STEPS AND SCHEDULER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa6021f-af27-47c0-bfe8-a600d1dd4ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fbbfa7-f5e9-40f6-8ba8-848432b15d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training steps is [number of batches] x [number of epochs].\n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = warmup_steps,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df252f-3368-447e-8310-259be0e75af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f65c4-018a-4eae-90a5-dcc4b1c01439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dcec4a-3477-4197-8c5e-d49124b9a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_t0 = time.time()\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(  b_input_ids,\n",
    "                          labels=b_labels,\n",
    "                          attention_mask = b_masks,\n",
    "                          token_type_ids=None\n",
    "                        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "\n",
    "        # Get sample every x batches.\n",
    "        if step % sample_every == 0 and not step == 0:\n",
    "\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "        #     print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "        #     sample_outputs = model.generate(\n",
    "        #                             bos_token_id=random.randint(1,30000),\n",
    "        #                             do_sample=True,\n",
    "        #                             top_k=50,\n",
    "        #                             max_length = 200,\n",
    "        #                             top_p=0.95,\n",
    "        #                             num_return_sequences=1\n",
    "        #                         )\n",
    "            # for i, sample_output in enumerate(sample_outputs):\n",
    "            #       print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs  = model(b_input_ids,\n",
    "#                            token_type_ids=None,\n",
    "                             attention_mask = b_masks,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd8315-0f2e-4543-98ca-57467e923604",
   "metadata": {},
   "source": [
    "##### SHOW THE TRAING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff0380-e2f3-4dd2-84d3-ad9238112524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display floats with two decimal places.\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ccb65-3f49-4cdd-9b19-1f9a8b73752b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e86e0-ca9f-477f-a6d7-f6afb8e7b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plot styling from seaborn.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf8775-0404-4106-95f1-62ff96e7d765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a4dfa7-9112-422a-a308-81163b99ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  Display Model Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7d81f2-5d7e-4700-a1e8-a6772eae7af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8194f0-c545-46cc-b89e-d6e6b783f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The GPT-2 model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:2]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[2:14]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-2:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c24c4bb-5361-4f75-a1b6-cbe8ab0817d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f184396-0974-437a-ad1c-a564c8475de5",
   "metadata": {},
   "source": [
    "##### Generate Text\n",
    "\n",
    "In below cell, you can change the question number from 0 - 374 . in result will show the question from the traning data. you also can change the question sentence to check the result of our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e591ed1b-5b9b-4d38-8541-ad3cb5ac0de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8203d-a64d-4d3b-887c-137fd46b2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define question number\n",
    "N = 333\n",
    "# GOOD :\n",
    "# NOT GOOD :34\n",
    "\n",
    "\n",
    "model.eval()\n",
    "# change a way to ask the question\n",
    "\n",
    "# question = 'which month did SDG & E begin commercial operation? '\n",
    "question = df['Q'].iloc[N,]\n",
    "\n",
    "\n",
    "prompt = '<|startoftext|>' + question + 'answer:'\n",
    "print('\\n\\n')\n",
    "print(df['Q'][N], df['A'][N])\n",
    "print('\\n')\n",
    "print('prompt:  '+ question)\n",
    "print('\\n')\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "generated = generated.to(device)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "                                generated,\n",
    "                                #bos_token_id=random.randint(1,30000),\n",
    "                                do_sample=True,\n",
    "                                top_k=50,\n",
    "                                max_length = 60,\n",
    "                                top_p=0.95,\n",
    "                                num_return_sequences=3\n",
    "                                )\n",
    "\n",
    "import re\n",
    "def extract_answer(text):\n",
    "    # Define a regex pattern to capture the information after \"answer:}\"\n",
    "    pattern = r'answer:}\\s*([^<]+)<|endoftext|>/.'\n",
    "\n",
    "\n",
    "\n",
    "    # Use re.search to find the pattern in the text\n",
    "    match = re.search(pattern, text)\n",
    "\n",
    "    # Check if the pattern was found\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  answer_text = tokenizer.decode(sample_output, skip_special_tokens=False)\n",
    "  result = extract_answer(answer_text)\n",
    "\n",
    "  print(\"{}: {}\\n\\n\".format(i, result))\n",
    "  # print(\"{}: {}\\n\\n\".format(i, answer_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b365d6fe-6086-4543-b03a-de520a81f8cb",
   "metadata": {},
   "source": [
    "Adding the result column to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f4f9ca-f0a7-4a58-8871-9c0b4d6e4138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "# N=3\n",
    "# # df['Q'][0]\n",
    "# print(f\"number: {N} ,  {df['Q'][N]}, {df['A'][N]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d13a725-f324-4088-8da7-0388dc6cf007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for loop to generate the answer list.\n",
    "fine_tune_answer = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for N,Q in enumerate(df['Q']):\n",
    "\n",
    "  question = Q\n",
    "\n",
    "  prompt = '<|startoftext|>' + question + 'answer:'\n",
    "\n",
    "\n",
    "  generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "  generated = generated.to(device)\n",
    "\n",
    "  sample_outputs = model.generate(\n",
    "                                  generated,\n",
    "                                  #bos_token_id=random.randint(1,30000),\n",
    "                                  do_sample=True,\n",
    "                                  top_k=50,\n",
    "                                  max_length = 50,\n",
    "                                  top_p=0.95,\n",
    "                                  # because we need to use only one result to adden in the column so set the number of return as 1.\n",
    "                                  num_return_sequences=1\n",
    "                                  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  for i, sample_output in enumerate(sample_outputs):\n",
    "    answer_text = tokenizer.decode(sample_output, skip_special_tokens=False)\n",
    "    result = extract_answer(answer_text)\n",
    "    # now add the fine_tuned_model answer to the list\n",
    "\n",
    "    fine_tune_answer.append(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca0ee2-e299-4a69-94ad-15e15deb092e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8713fa-4ab0-423d-88d6-0ad61e852c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the lenth is correct\n",
    "# len(df['A']) == len(fine_tune_answer)\n",
    "\n",
    "# ADD THE ANSWER TO df\n",
    "df['fine_tuned_answer'] = fine_tune_answer\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0582f29-4155-4a97-b058-7a84e1634ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf7d9e07-f943-4670-8667-cd3cb0df03ca",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "#### SET UP PINECONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51457e36-7327-4048-b624-eb316e705c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0c9e62-087a-453f-ba6e-f1dc904ebbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the sentence embedding database with pine-client\n",
    "!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a3b13-534c-4311-9743-0348904136ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b676d8-f6f9-49d0-adea-8c124160b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the packages\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ccd62-d63f-439d-a371-ee8f4de39db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097167da-8673-4f8e-81fe-5735f7d2b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registor your pipecone account, set up a index name as'clean-energy' with 384 dimentions.\n",
    "\n",
    "\n",
    "'''\n",
    "crearte 'clean_energy' index in Pinecone, set up the index dimention as 384.\n",
    "because i USE THE hugging face 'model sentence-transformers/paraphrase-MiniLM-L6-v2'\n",
    "\n",
    "\n",
    "here is the model stuctor:\n",
    "SentenceTransformer(\n",
    "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel\n",
    "  (1): Pooling({\n",
    "    'word_embedding_dimension': 384,\n",
    "    'pooling_mode_cls_token': False,\n",
    "    'pooling_mode_mean_tokens': True,\n",
    "    'pooling_mode_max_tokens': False,\n",
    "    'pooling_mode_mean_sqrt_len_tokens': False})\n",
    ")\n",
    "\n",
    "'''\n",
    "# use the api\n",
    "pc = Pinecone(api_key='d6e4416e-1947-4cbb-b77f-88efeee7183e')\n",
    "index_name = 'clean-energy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333dfb63-162b-42d9-8bdb-ae0c063b69a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed5b2e-1058-4c49-9c66-229b43ac1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK THE INDEX info, it should be empty\n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9aec6a-b4af-4c25-91a5-e83057140994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "632db143-1ed8-426b-8bde-75c0a7db6ba6",
   "metadata": {},
   "source": [
    "#### sentence embeding from stage3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e0a1a4-ee45-4ce9-92c6-1219cd7cd573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/sentence-transformers\n",
    "\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6098f03-35d5-49d0-88f2-d7cdeb8bf607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff59b1a-ea93-47a4-a787-509f15cc9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the df\n",
    "df1 = pd.read_csv(data_folder/'last100_QA.csv',index_col=0)\n",
    "\n",
    "#define the function to clean the text\n",
    "def clean_text_list(txt):\n",
    "  txt_list = txt[2:].split(\"', '\")\n",
    "  return [txt.strip(' ') for txt in txt_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9407e7-9b44-41fa-99e4-4abdb20f7182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda23f1c-8bd4-4e32-ba35-c853006aee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### prepare data for Pinecone\n",
    "\n",
    " - creating list of vector of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef05a4c-730e-452b-be9f-8a7b61a35219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b833d5-3654-4f73-a1ac-23222d3b755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = []\n",
    "original_dict = {}\n",
    "for i, txt in enumerate(df1['key_sentences']):\n",
    "  for j, s in enumerate(clean_text_list(txt)):\n",
    "    s_id = f'{i}_{j}'\n",
    "    s_value = se.encode(s).tolist()\n",
    "    original_dict[s_id] = s\n",
    "    embedding_list.append(\n",
    "        {\"id\": s_id, \"values\": s_value}\n",
    "    )\n",
    "print(len(embedding_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eaae3c-4ae6-4d39-8417-671e80773f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f0044ec-3a3c-4908-8ce6-5125f19b720a",
   "metadata": {},
   "source": [
    "#### UPSERT TO PIPECONE\n",
    "\n",
    "Use the upsert operation to write 100 384-dimensional vectors into pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c6dafd-608d-4056-a068-458fcb8e4436",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = \"last100\"\n",
    "index.upsert(\n",
    "  vectors=embedding_list,\n",
    "  namespace=ns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641d145-952b-49f6-a040-dd28511adbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164216b-0ee3-4d04-b298-bc909e95d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we have all vectors in index\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e86af-4ea7-4da6-add4-0d2ddd318c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa0b1b8-2012-4122-9f47-d939a0e6154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets context passages from the pinecone index\n",
    "def get_context(index, question, top_k):\n",
    "    # generate embeddings for the question\n",
    "    xq = se.encode(question).tolist()\n",
    "    # search pinecone index for context passage with the answer\n",
    "    xc = index.query(\n",
    "                      namespace=ns,\n",
    "                      vector=se.encode(question).tolist(),\n",
    "                      top_k=top_k,\n",
    "                      include_values=False\n",
    "                    )\n",
    "    # extract the context passage from pinecone search result\n",
    "    return xc[\"matches\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b6bf48-cc11-44e4-9e36-f1517fb85b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e9a23-aff1-46e3-845c-d019480afd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a quesiton and answer.\n",
    "df[['Q','A']].iloc[118]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b8958-2dac-4d67-bb58-36736bbd6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the question and query the from the embedding database\n",
    "\n",
    "question = \"Where is the Allen S. King plant located?\"\n",
    "context_list = get_context(index, question, top_k=5)\n",
    "\n",
    "context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b28647-5f74-4531-8f94-5e3778c33635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7af3d677-2f6a-46ac-94d8-377a13e4058e",
   "metadata": {},
   "source": [
    "#### Question-answering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d4263-8d2d-4323-8c8b-c11adc07d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/v4.17.0/en/task_summary#extractive-question-answering\n",
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cf3279-3b9e-4589-90cc-26526b719c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f00dc82-4480-463d-807d-6c618ea40093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "for c in context_list:\n",
    "  c_txt = original_dict[c['id']]\n",
    "  result = question_answerer(question=question, context=c_txt)\n",
    "  print()\n",
    "  print(f\"Embedding score: {c['score']}\")\n",
    "  print(\n",
    "    f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcf5b68-cd5f-491c-866f-5a252383f1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4606e2a5-75a1-448c-a00e-bd9f5d4b05c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84694ee2-8734-4fed-852e-59ba401c1b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02743b-61c4-41c7-a3ba-002a2af6c216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee4ea0-538f-4812-8deb-5511578229b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59ee3f-eea7-4123-be71-04aa188c2939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c59dd36-a9a8-439e-84fd-9f6cf59e006f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
