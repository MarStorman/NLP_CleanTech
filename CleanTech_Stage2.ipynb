{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b1c8be0-ec34-46a8-8a3d-41a8d33423e6",
   "metadata": {},
   "source": [
    "# Accelerating Cleantech Advancements through NLP-Powered Text Mining and Knowledge Extraction\n",
    "\n",
    "Group: Marusa Storman, Vignesh Govindaraj, Pradip Ravichandran"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1223dfd0-f463-43b2-b2e5-ef10107b5e34",
   "metadata": {},
   "source": [
    "## Stage 2: Advanced Embedding Models Training and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dfc9f1-ffe3-42d9-8853-3938da5510ea",
   "metadata": {},
   "source": [
    "### Data Preparation for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75622761-6c7b-4754-bebb-f3e847076d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing scipy==1.12...\n",
      "Requirement already satisfied: scipy==1.12 in c:\\users\\pradip\\switchdrive\\hslu\\_sem 4\\ctl\\venv\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in c:\\users\\pradip\\switchdrive\\hslu\\_sem 4\\ctl\\venv\\lib\\site-packages (from scipy==1.12) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the directory of the current notebook\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Change current working directory to where the notebook resides\n",
    "os.chdir(notebook_dir)\n",
    "\n",
    "# List of required libraries\n",
    "required_libraries = [\n",
    "    'gensim',\n",
    "    'scipy==1.12'\n",
    "    'transformers',\n",
    "    'torch'\n",
    "]\n",
    "\n",
    "# Check if each library is installed, if not, install it\n",
    "for lib in required_libraries:\n",
    "    try:\n",
    "        __import__(lib)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {lib}...\")\n",
    "        !\"{sys.executable}\" -m pip install {lib}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e211856-7f92-4519-8ebc-0a0d7d454c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "# Jupyter config\n",
    "%config InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6532d82-2262-41eb-a388-5fcb7d6a9b79",
   "metadata": {},
   "source": [
    "The dataset has already been cleaned and prepared for embedding training in the initial task. To streamline processing and avoid redundant code, we'll import the preprocessed data. Before splitting it, let's quickly review the data again as a reminder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de47158-2cd5-4a54-8e0a-1cf895656d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the preprocessed data from stage 1\n",
    "google_patent_original = pd.read_csv(\"Data/google_patent_en_preprocessed.csv\")\n",
    "media_original = pd.read_csv(\"Data/ct_media_preprocessed.csv\")\n",
    "media_evaluation_original = pd.read_csv(\"Data/ct_evaluation_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb279b8d-2ffe-4efa-ba1a-ebe647ee78d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Patent Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_number</th>\n",
       "      <th>country_code</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>title_localized_text</th>\n",
       "      <th>abstract_localized_text</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>abstract_tokens</th>\n",
       "      <th>title_token_count</th>\n",
       "      <th>abstract_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US-2022239235-A1</td>\n",
       "      <td>US</td>\n",
       "      <td>2022-07-28</td>\n",
       "      <td>adaptable dcac inverter drive system and opera...</td>\n",
       "      <td>disclosed is an adaptable dcac inverter system...</td>\n",
       "      <td>['adapt', 'dcac', 'invert', 'drive', 'system',...</td>\n",
       "      <td>['disclos', 'adapt', 'dcac', 'invert', 'system...</td>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US-2022239251-A1</td>\n",
       "      <td>US</td>\n",
       "      <td>2022-07-28</td>\n",
       "      <td>system for providing the energy from a single ...</td>\n",
       "      <td>in accordance with an example embodiment a sol...</td>\n",
       "      <td>['system', 'provid', 'energi', 'singl', 'conti...</td>\n",
       "      <td>['accord', 'exampl', 'embodi', 'solar', 'energ...</td>\n",
       "      <td>18</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US-11396827-B2</td>\n",
       "      <td>US</td>\n",
       "      <td>2022-07-26</td>\n",
       "      <td>control method for optimizing solartopower eff...</td>\n",
       "      <td>a control method for optimizing a solartopower...</td>\n",
       "      <td>['control', 'method', 'optim', 'solartopow', '...</td>\n",
       "      <td>['control', 'method', 'optim', 'solartopow', '...</td>\n",
       "      <td>15</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CN-114772674-A</td>\n",
       "      <td>CN</td>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>lowcarbon running saline wastewater treatment ...</td>\n",
       "      <td>the invention discloses a system and a method ...</td>\n",
       "      <td>['lowcarbon', 'run', 'salin', 'wastewat', 'tre...</td>\n",
       "      <td>['invent', 'disclos', 'system', 'method', 'tre...</td>\n",
       "      <td>15</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CN-217026795-U</td>\n",
       "      <td>CN</td>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>water ecological remediation device convenient...</td>\n",
       "      <td>the utility model discloses a water ecological...</td>\n",
       "      <td>['water', 'ecolog', 'remedi', 'devic', 'conven...</td>\n",
       "      <td>['util', 'model', 'disclos', 'water', 'ecolog'...</td>\n",
       "      <td>7</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publication_number country_code publication_date  \\\n",
       "0   US-2022239235-A1           US       2022-07-28   \n",
       "1   US-2022239251-A1           US       2022-07-28   \n",
       "2     US-11396827-B2           US       2022-07-26   \n",
       "3     CN-114772674-A           CN       2022-07-22   \n",
       "4     CN-217026795-U           CN       2022-07-22   \n",
       "\n",
       "                                title_localized_text  \\\n",
       "0  adaptable dcac inverter drive system and opera...   \n",
       "1  system for providing the energy from a single ...   \n",
       "2  control method for optimizing solartopower eff...   \n",
       "3  lowcarbon running saline wastewater treatment ...   \n",
       "4  water ecological remediation device convenient...   \n",
       "\n",
       "                             abstract_localized_text  \\\n",
       "0  disclosed is an adaptable dcac inverter system...   \n",
       "1  in accordance with an example embodiment a sol...   \n",
       "2  a control method for optimizing a solartopower...   \n",
       "3  the invention discloses a system and a method ...   \n",
       "4  the utility model discloses a water ecological...   \n",
       "\n",
       "                                        title_tokens  \\\n",
       "0  ['adapt', 'dcac', 'invert', 'drive', 'system',...   \n",
       "1  ['system', 'provid', 'energi', 'singl', 'conti...   \n",
       "2  ['control', 'method', 'optim', 'solartopow', '...   \n",
       "3  ['lowcarbon', 'run', 'salin', 'wastewat', 'tre...   \n",
       "4  ['water', 'ecolog', 'remedi', 'devic', 'conven...   \n",
       "\n",
       "                                     abstract_tokens  title_token_count  \\\n",
       "0  ['disclos', 'adapt', 'dcac', 'invert', 'system...                  7   \n",
       "1  ['accord', 'exampl', 'embodi', 'solar', 'energ...                 18   \n",
       "2  ['control', 'method', 'optim', 'solartopow', '...                 15   \n",
       "3  ['invent', 'disclos', 'system', 'method', 'tre...                 15   \n",
       "4  ['util', 'model', 'disclos', 'water', 'ecolog'...                  7   \n",
       "\n",
       "   abstract_token_count  \n",
       "0                    64  \n",
       "1                    92  \n",
       "2                   149  \n",
       "3                   226  \n",
       "4                   252  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Number of Entries</th>\n",
       "      <th>Missing/None Count</th>\n",
       "      <th>Uniqueness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>publication_number</th>\n",
       "      <td>object</td>\n",
       "      <td>13412</td>\n",
       "      <td>0</td>\n",
       "      <td>13351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country_code</th>\n",
       "      <td>object</td>\n",
       "      <td>13412</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publication_date</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>13412</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_localized_text</th>\n",
       "      <td>object</td>\n",
       "      <td>13412</td>\n",
       "      <td>0</td>\n",
       "      <td>12441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abstract_localized_text</th>\n",
       "      <td>object</td>\n",
       "      <td>13412</td>\n",
       "      <td>0</td>\n",
       "      <td>13250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_tokens</th>\n",
       "      <td>object</td>\n",
       "      <td>13412</td>\n",
       "      <td>0</td>\n",
       "      <td>12424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abstract_tokens</th>\n",
       "      <td>object</td>\n",
       "      <td>13412</td>\n",
       "      <td>0</td>\n",
       "      <td>13235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_token_count</th>\n",
       "      <td>int64</td>\n",
       "      <td>13412</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abstract_token_count</th>\n",
       "      <td>int64</td>\n",
       "      <td>13412</td>\n",
       "      <td>0</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Data Type  Number of Entries  \\\n",
       "publication_number               object              13412   \n",
       "country_code                     object              13412   \n",
       "publication_date         datetime64[ns]              13412   \n",
       "title_localized_text             object              13412   \n",
       "abstract_localized_text          object              13412   \n",
       "title_tokens                     object              13412   \n",
       "abstract_tokens                  object              13412   \n",
       "title_token_count                 int64              13412   \n",
       "abstract_token_count              int64              13412   \n",
       "\n",
       "                         Missing/None Count  Uniqueness  \n",
       "publication_number                        0       13351  \n",
       "country_code                              0          29  \n",
       "publication_date                          0         158  \n",
       "title_localized_text                      0       12441  \n",
       "abstract_localized_text                   0       13250  \n",
       "title_tokens                              0       12424  \n",
       "abstract_tokens                           0       13235  \n",
       "title_token_count                         0          30  \n",
       "abstract_token_count                      0         282  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# This function will provide with more useful information:\n",
    "def analyze_column(df, has_list=False):\n",
    "    info = pd.DataFrame({\n",
    "        'Data Type': df.dtypes,\n",
    "        'Number of Entries': df.count(),\n",
    "        'Missing/None Count': df.isna().sum(),\n",
    "        'Uniqueness': df.nunique()\n",
    "    })\n",
    "    \n",
    "    return info\n",
    "\n",
    "print(\"Google Patent Dataset:\")\n",
    "google_patent_original['publication_date'] = pd.to_datetime(google_patent_original['publication_date'])\n",
    "google_patent_original.head()\n",
    "analyze_column(google_patent_original)\n",
    "print(\"\\nNumber of duplicate rows:\", media_original.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12875001-2238-435a-ae28-6d90b57f404b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>domain</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>content_tokens</th>\n",
       "      <th>title_token_count</th>\n",
       "      <th>content_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qatar to slash emissions as lng expansion adva...</td>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>qatar petroleum qp is targeting aggressive cut...</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>['qatar', 'slash', 'emiss', 'lng', 'expans', '...</td>\n",
       "      <td>['qatar', 'petroleum', 'qp', 'target', 'aggres...</td>\n",
       "      <td>8</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>india launches its first 700 mw phwr</td>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>nuclear power corp of india ltd npcil synchro...</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>['india', 'launch', 'first', '700', 'mw', 'phwr']</td>\n",
       "      <td>['nuclear', 'power', 'corp', 'india', 'ltd', '...</td>\n",
       "      <td>7</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new chapter for uschina energy trade</td>\n",
       "      <td>2021-01-20</td>\n",
       "      <td>new us president joe biden took office this we...</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>['new', 'chapter', 'uschina', 'energi', 'trade']</td>\n",
       "      <td>['new', 'presid', 'joe', 'biden', 'took', 'off...</td>\n",
       "      <td>6</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>japan slow restarts cast doubt on 2030 energy ...</td>\n",
       "      <td>2021-01-22</td>\n",
       "      <td>the slow pace of japanese reactor restarts con...</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>['japan', 'slow', 'restart', 'cast', 'doubt', ...</td>\n",
       "      <td>['slow', 'pace', 'japanes', 'reactor', 'restar...</td>\n",
       "      <td>9</td>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nyc pension funds to divest fossil fuel shares</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>two of new york citys largest pension funds sa...</td>\n",
       "      <td>energyintel</td>\n",
       "      <td>['nyc', 'pension', 'fund', 'divest', 'fossil',...</td>\n",
       "      <td>['two', 'new', 'york', 'citi', 'largest', 'pen...</td>\n",
       "      <td>8</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title       date  \\\n",
       "0  qatar to slash emissions as lng expansion adva... 2021-01-13   \n",
       "1               india launches its first 700 mw phwr 2021-01-15   \n",
       "2               new chapter for uschina energy trade 2021-01-20   \n",
       "3  japan slow restarts cast doubt on 2030 energy ... 2021-01-22   \n",
       "4     nyc pension funds to divest fossil fuel shares 2021-01-25   \n",
       "\n",
       "                                             content       domain  \\\n",
       "0  qatar petroleum qp is targeting aggressive cut...  energyintel   \n",
       "1   nuclear power corp of india ltd npcil synchro...  energyintel   \n",
       "2  new us president joe biden took office this we...  energyintel   \n",
       "3  the slow pace of japanese reactor restarts con...  energyintel   \n",
       "4  two of new york citys largest pension funds sa...  energyintel   \n",
       "\n",
       "                                        title_tokens  \\\n",
       "0  ['qatar', 'slash', 'emiss', 'lng', 'expans', '...   \n",
       "1  ['india', 'launch', 'first', '700', 'mw', 'phwr']   \n",
       "2   ['new', 'chapter', 'uschina', 'energi', 'trade']   \n",
       "3  ['japan', 'slow', 'restart', 'cast', 'doubt', ...   \n",
       "4  ['nyc', 'pension', 'fund', 'divest', 'fossil',...   \n",
       "\n",
       "                                      content_tokens  title_token_count  \\\n",
       "0  ['qatar', 'petroleum', 'qp', 'target', 'aggres...                  8   \n",
       "1  ['nuclear', 'power', 'corp', 'india', 'ltd', '...                  7   \n",
       "2  ['new', 'presid', 'joe', 'biden', 'took', 'off...                  6   \n",
       "3  ['slow', 'pace', 'japanes', 'reactor', 'restar...                  9   \n",
       "4  ['two', 'new', 'york', 'citi', 'largest', 'pen...                  8   \n",
       "\n",
       "   content_token_count  \n",
       "0                  442  \n",
       "1                  538  \n",
       "2                  706  \n",
       "3                  687  \n",
       "4                  394  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Number of Entries</th>\n",
       "      <th>Missing/None Count</th>\n",
       "      <th>Uniqueness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>object</td>\n",
       "      <td>9593</td>\n",
       "      <td>0</td>\n",
       "      <td>9565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>9593</td>\n",
       "      <td>0</td>\n",
       "      <td>967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content</th>\n",
       "      <td>object</td>\n",
       "      <td>9593</td>\n",
       "      <td>0</td>\n",
       "      <td>9588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain</th>\n",
       "      <td>object</td>\n",
       "      <td>9593</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_tokens</th>\n",
       "      <td>object</td>\n",
       "      <td>9593</td>\n",
       "      <td>0</td>\n",
       "      <td>9563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_tokens</th>\n",
       "      <td>object</td>\n",
       "      <td>9593</td>\n",
       "      <td>0</td>\n",
       "      <td>9587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_token_count</th>\n",
       "      <td>int64</td>\n",
       "      <td>9593</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_token_count</th>\n",
       "      <td>int64</td>\n",
       "      <td>9593</td>\n",
       "      <td>0</td>\n",
       "      <td>1782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Data Type  Number of Entries  Missing/None Count  \\\n",
       "title                        object               9593                   0   \n",
       "date                 datetime64[ns]               9593                   0   \n",
       "content                      object               9593                   0   \n",
       "domain                       object               9593                   0   \n",
       "title_tokens                 object               9593                   0   \n",
       "content_tokens               object               9593                   0   \n",
       "title_token_count             int64               9593                   0   \n",
       "content_token_count           int64               9593                   0   \n",
       "\n",
       "                     Uniqueness  \n",
       "title                      9565  \n",
       "date                        967  \n",
       "content                    9588  \n",
       "domain                       19  \n",
       "title_tokens               9563  \n",
       "content_tokens             9587  \n",
       "title_token_count            25  \n",
       "content_token_count        1782  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Media Dataset:\")\n",
    "media_original['date'] = pd.to_datetime(media_original['date'])\n",
    "media_original.head()\n",
    "analyze_column(media_original)\n",
    "print(\"\\nNumber of duplicate rows:\", media_original.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e46fde-784b-4279-9c0e-9ea935f88e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media Evaluation Dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>relevant_chunk</th>\n",
       "      <th>domain</th>\n",
       "      <th>question_tokens</th>\n",
       "      <th>relevant_chunk_tokens</th>\n",
       "      <th>question_token_count</th>\n",
       "      <th>relevant_chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>what is the innovation behind leclanches new m...</td>\n",
       "      <td>leclanche said it has developed an environment...</td>\n",
       "      <td>sgvoice.net</td>\n",
       "      <td>['innov', 'behind', 'leclanch', 'new', 'method...</td>\n",
       "      <td>['leclanch', 'said', 'develop', 'environment',...</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>what is the eus green deal industrial plan</td>\n",
       "      <td>the green deal industrial plan is a bid by the...</td>\n",
       "      <td>sgvoice.net</td>\n",
       "      <td>['eu', 'green', 'deal', 'industri', 'plan']</td>\n",
       "      <td>['green', 'deal', 'industri', 'plan', 'bid', '...</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>what is the eus green deal industrial plan</td>\n",
       "      <td>the european counterpart to the us inflation r...</td>\n",
       "      <td>pv-magazine.com</td>\n",
       "      <td>['eu', 'green', 'deal', 'industri', 'plan']</td>\n",
       "      <td>['european', 'counterpart', 'inflat', 'reduct'...</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>what are the four focus areas of the eus green...</td>\n",
       "      <td>the new plan is fundamentally focused on four ...</td>\n",
       "      <td>sgvoice.net</td>\n",
       "      <td>['four', 'focu', 'area', 'eu', 'green', 'deal'...</td>\n",
       "      <td>['new', 'plan', 'fundament', 'focus', 'four', ...</td>\n",
       "      <td>13</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>when did the cooperation between gm and honda ...</td>\n",
       "      <td>what caught our eye was a new hookup between g...</td>\n",
       "      <td>cleantechnica.com</td>\n",
       "      <td>['cooper', 'gm', 'honda', 'fuel', 'cell', 'veh...</td>\n",
       "      <td>['caught', 'eye', 'new', 'hookup', 'gm', 'hond...</td>\n",
       "      <td>13</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example_id  question_id                                           question  \\\n",
       "0           1            1  what is the innovation behind leclanches new m...   \n",
       "1           2            2         what is the eus green deal industrial plan   \n",
       "2           3            2         what is the eus green deal industrial plan   \n",
       "3           4            3  what are the four focus areas of the eus green...   \n",
       "4           5            4  when did the cooperation between gm and honda ...   \n",
       "\n",
       "                                      relevant_chunk             domain  \\\n",
       "0  leclanche said it has developed an environment...        sgvoice.net   \n",
       "1  the green deal industrial plan is a bid by the...        sgvoice.net   \n",
       "2  the european counterpart to the us inflation r...    pv-magazine.com   \n",
       "3  the new plan is fundamentally focused on four ...        sgvoice.net   \n",
       "4  what caught our eye was a new hookup between g...  cleantechnica.com   \n",
       "\n",
       "                                     question_tokens  \\\n",
       "0  ['innov', 'behind', 'leclanch', 'new', 'method...   \n",
       "1        ['eu', 'green', 'deal', 'industri', 'plan']   \n",
       "2        ['eu', 'green', 'deal', 'industri', 'plan']   \n",
       "3  ['four', 'focu', 'area', 'eu', 'green', 'deal'...   \n",
       "4  ['cooper', 'gm', 'honda', 'fuel', 'cell', 'veh...   \n",
       "\n",
       "                               relevant_chunk_tokens  question_token_count  \\\n",
       "0  ['leclanch', 'said', 'develop', 'environment',...                    12   \n",
       "1  ['green', 'deal', 'industri', 'plan', 'bid', '...                     8   \n",
       "2  ['european', 'counterpart', 'inflat', 'reduct'...                     8   \n",
       "3  ['new', 'plan', 'fundament', 'focus', 'four', ...                    13   \n",
       "4  ['caught', 'eye', 'new', 'hookup', 'gm', 'hond...                    13   \n",
       "\n",
       "   relevant_chunk_token_count  \n",
       "0                          36  \n",
       "1                          47  \n",
       "2                          35  \n",
       "3                          42  \n",
       "4                          60  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Number of Entries</th>\n",
       "      <th>Missing/None Count</th>\n",
       "      <th>Uniqueness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>example_id</th>\n",
       "      <td>int64</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_id</th>\n",
       "      <td>int64</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>object</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relevant_chunk</th>\n",
       "      <td>object</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain</th>\n",
       "      <td>object</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_tokens</th>\n",
       "      <td>object</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relevant_chunk_tokens</th>\n",
       "      <td>object</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_token_count</th>\n",
       "      <td>int64</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relevant_chunk_token_count</th>\n",
       "      <td>int64</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Data Type  Number of Entries  Missing/None Count  \\\n",
       "example_id                     int64                 23                   0   \n",
       "question_id                    int64                 23                   0   \n",
       "question                      object                 23                   0   \n",
       "relevant_chunk                object                 23                   0   \n",
       "domain                        object                 23                   0   \n",
       "question_tokens               object                 23                   0   \n",
       "relevant_chunk_tokens         object                 23                   0   \n",
       "question_token_count           int64                 23                   0   \n",
       "relevant_chunk_token_count     int64                 23                   0   \n",
       "\n",
       "                            Uniqueness  \n",
       "example_id                          23  \n",
       "question_id                         21  \n",
       "question                            21  \n",
       "relevant_chunk                      23  \n",
       "domain                               6  \n",
       "question_tokens                     21  \n",
       "relevant_chunk_tokens               23  \n",
       "question_token_count                12  \n",
       "relevant_chunk_token_count          18  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Media Evaluation Dataset:\")\n",
    "media_evaluation_original.head()\n",
    "analyze_column(media_evaluation_original)\n",
    "print(\"\\nNumber of duplicate rows:\", media_evaluation_original.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf89aa-3187-49f6-9e8e-6c304a287e11",
   "metadata": {},
   "source": [
    "To ensure unique characteristics and fair splitting, we will prioritize the \"country\" column for patents. This approach ensures each country is proportionally represented in both the test and validation datasets. We chose the country column because it is the only attribute that makes sense; the type of patent may be influenced by its country of origin.\n",
    "\n",
    "For countries with only one patent, we cannot split them effectively. Therefore, we created a new \"Country\" tag, \"OT\" (short for \"other\"), to group these countries together and facilitate splitting.\n",
    "\n",
    "For the media dataset, we chose the \"domain\" column. Similarly, a domain may report on specific topics or emphasize certain aspects, making it important for the dataset's characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bb5aa98-ba1b-4993-9563-4ea3a97032ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the rows with an unique country_code\n",
    "class_counts = google_patent_original['country_code'].value_counts()\n",
    "single_instances = class_counts[class_counts == 1].index.tolist()\n",
    "\n",
    "# Update country_code for single-instance classes\n",
    "google_patent_original.loc[google_patent_original['country_code'].isin(single_instances), 'country_code'] = 'OT'  # OT = Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d98ac87-8af7-4e2f-b8bf-bad364e0ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split patent data into training and validation sets, country code is been splitted equal\n",
    "patent_train, patent_val = train_test_split(google_patent_original, test_size=0.2, stratify=google_patent_original['country_code'], random_state=42)\n",
    "\n",
    "# Split media data into training and validation sets, domain is been splitted equal\n",
    "media_train, media_val = train_test_split(media_original, test_size=0.2, stratify=media_original['domain'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ee522-d304-4be6-83cc-285f52549d15",
   "metadata": {},
   "source": [
    "### Word Embedding Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a031c4-0d12-41da-ba23-228528c2b5d6",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d360c78f-040b-4a1f-9b29-1feaff405cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenized_data(data):\n",
    "    return [ast.literal_eval(sentence) for sentence in data]\n",
    "\n",
    "# Function to train Word2Vec model\n",
    "def train_word2vec_model(data, vector_size=100, window=5, epochs=10):\n",
    "    model = Word2Vec(sentences=data, vector_size=vector_size, window=window, epochs=epochs)\n",
    "    return model\n",
    "\n",
    "# Define parameters for training and evaluation\n",
    "parameters = {\n",
    "    'vector_size': [100, 200],\n",
    "    'window': [5, 10],\n",
    "    'epochs': [10, 20]\n",
    "}\n",
    "\n",
    "def hypertrain(model_basename, train_data, sub_folder):\n",
    "    models: dict[str, Word2Vec] = {}\n",
    "    for vector_size in parameters['vector_size']:\n",
    "        for window in parameters['window']:\n",
    "            for epochs in parameters['epochs']:\n",
    "                # Train Word2Vec model\n",
    "                model_name = f'{model_basename}_{vector_size}_{window}_{epochs}'\n",
    "                print(f'Training Word2Vec model {model_name} ...')\n",
    "                model = train_word2vec_model(train_data, vector_size=vector_size, window=window, epochs=epochs)\n",
    "\n",
    "                model.save(f'Data/Word/{sub_folder}/{model_name}.model')\n",
    "                models[model_name] = model\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac4e680-a22b-4484-ba76-2d8ce79ce82e",
   "metadata": {},
   "source": [
    "We attempted several evaluation methods, but since they all yielded the same results, we have decided to discontinue them to save execution time."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad463de1-5631-43c3-a56f-26f120899dbd",
   "metadata": {},
   "source": [
    "# Function to evaluate all models and choose the best one\n",
    "def evaluate_all_models(models, data, labels):\n",
    "    best_model_name = None\n",
    "    best_model_performance = 0\n",
    "    results = []\n",
    "    # Loop through each model\n",
    "    for model_name, model in models.items():\n",
    "        # Prepare data\n",
    "        X = np.array([get_text_embedding(model, tokens) for tokens in data])\n",
    "        # Train-test split\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "        # Train classifier\n",
    "        classifier = LogisticRegression(max_iter=1000)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        # Predict and evaluate\n",
    "        y_pred = classifier.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred, average='weighted')\n",
    "        recall = recall_score(y_val, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "        # Collect results\n",
    "        results.append((model_name, accuracy, precision, recall, f1))\n",
    "        # Check if this is the best model\n",
    "        if f1 > best_model_performance:\n",
    "            best_model_performance = f1\n",
    "            best_model_name = model_name\n",
    "    # Print results for all models\n",
    "    for result in results:\n",
    "        print(f\"Model: {result[0]}, Accuracy: {result[1]}, Precision: {result[2]}, Recall: {result[3]}, F1 Score: {result[4]}\")\n",
    "    print(f\"\\nBest model: {best_model_name} with F1 Score: {best_model_performance}\")\n",
    "    return best_model_name, best_model_performance\n",
    "\n",
    "# Function to get embeddings for a text\n",
    "def get_text_embedding(model, text_tokens):\n",
    "    vector_size = model.vector_size\n",
    "    embedding = np.zeros(vector_size)\n",
    "    n_words = 0\n",
    "    for token in text_tokens:\n",
    "        if token in model.wv:\n",
    "            embedding += model.wv[token]\n",
    "            n_words += 1\n",
    "    if n_words > 0:\n",
    "        embedding /= n_words\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3396d8-12b3-4cfa-b4da-2c52bbd0cc06",
   "metadata": {},
   "source": [
    "##### Google patent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c1c1de9-e19b-4d2d-9cb7-0c8351d83a32",
   "metadata": {},
   "source": [
    "labels = patent_val['country_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b219c-9311-4ab4-a79d-26a5ad0febd9",
   "metadata": {},
   "source": [
    "###### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed901f35-f1f7-4a33-a049-0b38a8eecd2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model patent_title_100_5_10 ...\n",
      "Training Word2Vec model patent_title_100_5_20 ...\n",
      "Training Word2Vec model patent_title_100_10_10 ...\n",
      "Training Word2Vec model patent_title_100_10_20 ...\n",
      "Training Word2Vec model patent_title_200_5_10 ...\n",
      "Training Word2Vec model patent_title_200_5_20 ...\n",
      "Training Word2Vec model patent_title_200_10_10 ...\n",
      "Training Word2Vec model patent_title_200_10_20 ...\n"
     ]
    }
   ],
   "source": [
    "# Define train and validation data\n",
    "patent_word_title_train_data = clean_tokenized_data(patent_train['title_tokens'].tolist())\n",
    "patent_word_title_validation_data = patent_val['title_tokens']\n",
    "\n",
    "# Train and validate Word2Vec models\n",
    "patent_word_title_models = hypertrain(\"patent_title\", patent_word_title_train_data, \"Patent/Title\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23091376-0850-4c76-8521-355a07ac871a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Evaluate models for media titles\n",
    "print(\"Evaluating models for google patents title...\")\n",
    "best_patent_title_model, best_patent_title_performance = evaluate_all_models(patent_word_title_models, patent_word_title_validation_data, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df5a19c-666d-4618-8885-b0f9b4f96f62",
   "metadata": {},
   "source": [
    "###### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d253e33-8c50-41e8-827b-734072d09221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model patent_abstract_100_5_10 ...\n",
      "Training Word2Vec model patent_abstract_100_5_20 ...\n",
      "Training Word2Vec model patent_abstract_100_10_10 ...\n",
      "Training Word2Vec model patent_abstract_100_10_20 ...\n",
      "Training Word2Vec model patent_abstract_200_5_10 ...\n",
      "Training Word2Vec model patent_abstract_200_5_20 ...\n",
      "Training Word2Vec model patent_abstract_200_10_10 ...\n",
      "Training Word2Vec model patent_abstract_200_10_20 ...\n"
     ]
    }
   ],
   "source": [
    "# Define train and validation data\n",
    "patent_word_abstract_train_data = clean_tokenized_data(patent_train['abstract_tokens'].tolist())\n",
    "patent_word_abstract_validation_data = patent_val['abstract_tokens']\n",
    "\n",
    "# Train and validate Word2Vec models\n",
    "patent_word_abstract_models = hypertrain(\"patent_abstract\", patent_word_abstract_train_data, \"Patent/Text\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23b1dacc-fac6-41d3-971e-2b2259b9e095",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Evaluate models for media content\n",
    "print(\"\\nEvaluating models for google patents abstract...\")\n",
    "best_patent_abstract_model, best_patent_abstract_performance = evaluate_all_models(patent_word_abstract_models, patent_word_abstract_validation_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f20e540-3865-454a-8ac0-9b93a8381181",
   "metadata": {},
   "source": [
    "##### Cleantech Media"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d04071-8e1b-405b-a5bb-3daec1a09fea",
   "metadata": {},
   "source": [
    "###### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8cb7509-d9e8-4473-95bd-affa2ac65d76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model media_title_100_5_10 ...\n",
      "Training Word2Vec model media_title_100_5_20 ...\n",
      "Training Word2Vec model media_title_100_10_10 ...\n",
      "Training Word2Vec model media_title_100_10_20 ...\n",
      "Training Word2Vec model media_title_200_5_10 ...\n",
      "Training Word2Vec model media_title_200_5_20 ...\n",
      "Training Word2Vec model media_title_200_10_10 ...\n",
      "Training Word2Vec model media_title_200_10_20 ...\n"
     ]
    }
   ],
   "source": [
    "# Define train and validation data\n",
    "train_data = clean_tokenized_data(media_train['title_tokens'].tolist())\n",
    "validation_data = media_train['title_tokens']\n",
    "\n",
    "# Train and validate Word2Vec models\n",
    "media_word_title_models = hypertrain(\"media_title\", train_data, \"Media/Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f14e3-833f-48d6-8f65-a13c3ce1609c",
   "metadata": {},
   "source": [
    "###### Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8694defb-95a4-4aad-87f7-839a4fbed519",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model media_content_100_5_10 ...\n",
      "Training Word2Vec model media_content_100_5_20 ...\n",
      "Training Word2Vec model media_content_100_10_10 ...\n",
      "Training Word2Vec model media_content_100_10_20 ...\n",
      "Training Word2Vec model media_content_200_5_10 ...\n",
      "Training Word2Vec model media_content_200_5_20 ...\n",
      "Training Word2Vec model media_content_200_10_10 ...\n",
      "Training Word2Vec model media_content_200_10_20 ...\n"
     ]
    }
   ],
   "source": [
    "# Define train and validation data\n",
    "train_data = clean_tokenized_data(media_train['content_tokens'].tolist())\n",
    "validation_data = media_train['content_tokens']\n",
    "\n",
    "# Train and validate Word2Vec models\n",
    "media_word_content_models = hypertrain(\"media_content\", train_data, \"Media/Text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b2c31a-9330-4fb3-8879-905af71b9fc5",
   "metadata": {},
   "source": [
    "### Sentence Embedding Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854de4a-85a0-4925-9a52-c50f099dc504",
   "metadata": {},
   "source": [
    "Here again we tried several ways, this one is the best and most opitmized one, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b5f06b7-33e8-4944-a318-806edfed8942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sentences, tokenizer, model, device='cuda', max_length=512, batch_size=128):\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    num_batches = len(sentences) // batch_size + (1 if len(sentences) % batch_size != 0 else 0)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Get the embeddings from the [CLS] token (first token)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.extend(cls_embeddings)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        if (i // batch_size) % 10 == 0:  # Print progress every 10 batches\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Processed batch {i // batch_size + 1}/{num_batches}, elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428fa0c7-8e81-4c75-b942-9df3db9338fe",
   "metadata": {},
   "source": [
    "#### Google Patent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f230329-290c-4e10-8ba0-4820e0f9189e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Define the columns to be used\n",
    "title_column = 'title_localized_text'\n",
    "\n",
    "# Encode titles and abstracts for training and validation sets\n",
    "patent_train_titles = encode_sentences(patent_train[title_column].tolist(), bert_tokenizer, bert_model)\n",
    "patent_val_titles = encode_sentences(patent_val[title_column].tolist(), bert_tokenizer, bert_model)\n",
    "\n",
    "# Convert lists to DataFrames for easier handling\n",
    "patent_train_titles_df = pd.DataFrame(patent_train_titles)\n",
    "patent_val_titles_df = pd.DataFrame(patent_val_titles)\n",
    "\n",
    "# Save embeddings to files if needed\n",
    "patent_train_titles_df.to_csv('Data/Sentence/google_patent_en_train_titles_embeddings.csv', index=False)\n",
    "patent_val_titles_df.to_csv('Data/Sentence/google_patent_en_val_titles_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2809511-3bbe-4a9f-b279-69f496db64aa",
   "metadata": {},
   "source": [
    "# Define the columns to be used\n",
    "abstract_column = 'abstract_localized_text'\n",
    "\n",
    "# Encode titles and abstracts for training and validation sets\n",
    "patent_train_abstracts = encode_sentences(patent_train[abstract_column].tolist(), bert_tokenizer, bert_model)\n",
    "patent_val_abstracts = encode_sentences(patent_val[abstract_column].tolist(), bert_tokenizer, bert_model)\n",
    "\n",
    "# Convert lists to DataFrames for easier handling\n",
    "patent_train_abstracts_df = pd.DataFrame(patent_train_abstracts)\n",
    "patent_val_abstracts_df = pd.DataFrame(patent_val_abstracts)\n",
    "\n",
    "# Save embeddings to files if needed\n",
    "patent_train_abstracts_df.to_csv('Data/Sentence/google_patent_en_train_abstracts_embeddings.csv', index=False)\n",
    "patent_val_abstracts_df.to_csv('Data/Sentence/google_patent_en_val_abstracts_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e2797b-e4da-41bd-b280-82b5c5add15b",
   "metadata": {},
   "source": [
    "Due to the long process time we just let it run once (except we have some changes) and import it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12d9d439-eea9-4600-bf9e-278650052923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings from CSV files into DataFrames\n",
    "patent_train_titles_df = pd.read_csv('Data/Sentence/google_patent_en_train_titles_embeddings.csv')\n",
    "patent_val_titles_df = pd.read_csv('Data/Sentence/google_patent_en_val_titles_embeddings.csv')\n",
    "patent_train_abstracts_df = pd.read_csv('Data/Sentence/google_patent_en_train_abstracts_embeddings.csv')\n",
    "patent_val_abstracts_df = pd.read_csv('Data/Sentence/google_patent_en_val_abstracts_embeddings.csv')\n",
    "\n",
    "# Merge the embeddings DataFrame with the original patent_train DataFrame\n",
    "patent_train_titles = pd.concat([patent_train, patent_train_titles_df], axis=1)\n",
    "patent_train.rename(columns=lambda x: 'title_embedding' if 'Unnamed' in x else x, inplace=True)\n",
    "patent_train = pd.concat([patent_train, patent_train_abstracts_df], axis=1)\n",
    "patent_train.rename(columns=lambda x: 'abstract_embedding' if 'Unnamed' in x else x, inplace=True)\n",
    "\n",
    "patent_val = pd.concat([patent_val, patent_val_titles_df], axis=1)\n",
    "patent_val.rename(columns=lambda x: 'title_embedding' if 'Unnamed' in x else x, inplace=True)\n",
    "patent_val = pd.concat([patent_val, patent_val_abstracts_df], axis=1)\n",
    "patent_val.rename(columns=lambda x: 'abstract_embedding' if 'Unnamed' in x else x, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dedcd260-b1c6-4648-8fe9-6b2b2fe94861",
   "metadata": {},
   "source": [
    "# TODO: Not working anymore, fix it\n",
    "\n",
    "# Extract labels\n",
    "label_column = \"country_code\" # TODO: find a better solution or other label\n",
    "train_labels = patent_train_titles[label_column]\n",
    "val_labels = patent_val[label_column]\n",
    "\n",
    "# Define the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(patent_train_titles, train_labels)\n",
    "\n",
    "# Predict on validation set\n",
    "val_predictions = model.predict(patent_val)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(val_labels, patent_val)\n",
    "r2 = r2_score(val_labels, val_predictions)\n",
    "print(f'Validation Mean Squared Error: {mse}')\n",
    "print(f'Validation R^2 Score: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cff85a-7350-4e94-ac73-a6972f0615df",
   "metadata": {},
   "source": [
    "#### Cleantech Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "470577c2-1769-4e54-b569-78bf7489e7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/60, elapsed time: 4.63 seconds\n",
      "Processed batch 11/60, elapsed time: 41.52 seconds\n",
      "Processed batch 21/60, elapsed time: 83.83 seconds\n",
      "Processed batch 31/60, elapsed time: 132.45 seconds\n",
      "Processed batch 41/60, elapsed time: 176.25 seconds\n",
      "Processed batch 51/60, elapsed time: 220.50 seconds\n",
      "Processed batch 1/15, elapsed time: 4.68 seconds\n",
      "Processed batch 11/15, elapsed time: 50.05 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the columns to be used\n",
    "title_column = 'title'\n",
    "\n",
    "# Encode titles and abstracts for training and validation sets\n",
    "media_train_titles = encode_sentences(media_train[title_column].tolist(), bert_tokenizer, bert_model)\n",
    "media_val_titles = encode_sentences(media_val[title_column].tolist(), bert_tokenizer, bert_model)\n",
    "\n",
    "# Convert lists to DataFrames for easier handling\n",
    "media_train_titles_df = pd.DataFrame(media_train_titles)\n",
    "media_val_titles_df = pd.DataFrame(media_val_titles)\n",
    "\n",
    "# Save embeddings to files if needed\n",
    "media_train_titles_df.to_csv('Data/Sentence/media_train_titles_embeddings.csv', index=False)\n",
    "media_val_titles_df.to_csv('Data/Sentence/media_val_titles_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1abc5283-38a9-4cfc-b975-cba49e2275c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/60, elapsed time: 96.20 seconds\n",
      "Processed batch 11/60, elapsed time: 1092.52 seconds\n",
      "Processed batch 21/60, elapsed time: 2336.34 seconds\n",
      "Processed batch 31/60, elapsed time: 3528.26 seconds\n",
      "Processed batch 41/60, elapsed time: 35542.99 seconds\n",
      "Processed batch 51/60, elapsed time: 38919.86 seconds\n",
      "Processed batch 1/15, elapsed time: 52317.23 seconds\n",
      "Processed batch 11/15, elapsed time: 53533.41 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the columns to be used\n",
    "title_column = 'content'\n",
    "\n",
    "# Encode titles and abstracts for training and validation sets\n",
    "media_train_content = encode_sentences(media_train[title_column].tolist(), bert_tokenizer, bert_model)\n",
    "media_val_content = encode_sentences(media_val[title_column].tolist(), bert_tokenizer, bert_model)\n",
    "\n",
    "# Convert lists to DataFrames for easier handling\n",
    "media_train_content_df = pd.DataFrame(media_train_content)\n",
    "media_val_content_df = pd.DataFrame(media_val_content)\n",
    "\n",
    "# Save embeddings to files if needed\n",
    "media_train_content_df.to_csv('Data/Sentence/media_train_content_embeddings.csv', index=False)\n",
    "media_val_content_df.to_csv('Data/Sentence/media_val_content_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3614f63-e101-44ce-91b0-d66894f4dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings from CSV files into DataFrames\n",
    "media_train_titles_df = pd.read_csv('Data/Sentence/media_train_titles_embeddings.csv')\n",
    "media_val_titles_df = pd.read_csv('Data/Sentence/media_val_titles_embeddings.csv')\n",
    "media_train_abstracts_df = pd.read_csv('Data/Sentence/media_train_content_embeddings.csv')\n",
    "media_val_abstracts_df = pd.read_csv('Data/Sentence/media_val_content_embeddings.csv')\n",
    "\n",
    "# Merge the embeddings DataFrame with the original patent_train DataFrame\n",
    "media_train_titles = pd.concat([media_train, media_train_titles_df], axis=1)\n",
    "media_train.rename(columns=lambda x: 'title_embedding' if 'Unnamed' in x else x, inplace=True)\n",
    "media_train = pd.concat([media_train, media_train_abstracts_df], axis=1)\n",
    "media_train.rename(columns=lambda x: 'content_embedding' if 'Unnamed' in x else x, inplace=True)\n",
    "\n",
    "media_val = pd.concat([media_val, media_val_titles_df], axis=1)\n",
    "media_val.rename(columns=lambda x: 'title_embedding' if 'Unnamed' in x else x, inplace=True)\n",
    "media_val = pd.concat([media_val, media_val_abstracts_df], axis=1)\n",
    "media_val.rename(columns=lambda x: 'content_embedding' if 'Unnamed' in x else x, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
